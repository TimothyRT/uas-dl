{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyek UAS (Generasi Haiku dengan Fine-tuning GPT-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 222101862, Gery Yulianto, IBDA\n",
    "\n",
    "### 222102303, Jonathan Febrian Indrajaya Handoyo, IBDA\n",
    "\n",
    "### 222101412, Timothy Rudolf Tan, IBDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from transformers import (\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I - Latar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Haiku adalah suatu jenis puisi singkat yang berasal dari Jepangâ€”umumnya bertemakan alam, tetapi bisa pula mengambil topik lain. Keunikan dari haiku sendiri terletak pada komposisinya yang mencakup tiga baris pendek; secara tradisional, haiku semestinya mengikuti pola 5-7-5 (suku kata per baris). Haiku modern namun tidak lagi terikat ketat aturan semacam ini, walau biasanya masih dibatasi agar memiliki total sekitar 10/12 kata saja.\n",
    "\n",
    "Semenjak akhir abad ke-19, minat masyarakat global terhadap pembuatan haiku telah berkembang, dengan bahasa Inggris sebagai salah satu favorit alternatif bahasa Jepang untuk pembuatan haiku. Dunia pemrosesan bahasa natural sendiri telah memperkenalkan sebuah metode unik untuk menghasilkan haiku tanpa melibatkan langsung sosok manusia, yakni menggunakan generative AI. Upaya awal pemakaian neural network untuk memproduksi haiku Jepang dikerjakan oleh Wu et al. (2017), yang mencoba empat arsitektur berbeda: recurrent neural network (RNN), RNN berbasis gated recurrent unit (GRU) dan long short-term memory (LSTM), recurrent convolutional neural network (RCNN), serta sequence generative adversarial network (SeqGAN). Beberapa tahun belakangan, pendekatan large language model (LLM) telah pula dicoba. Model Haikoo buatan Miceli (2021), misalnya, menggabungkan model GPT-2 dengan plug and play language model (PPLM) untuk menghasilkan haiku Inggris berdasarkan kata kunci spesifik.\n",
    "\n",
    "Dalam proyek ini sendiri, kami tertarik untuk meneruskan upaya pengembangan pendekatan LLM dalam pembuatan haiku bahasa Inggris. Di sini, kami hendak melakukan fine-tuning pada model GPT-2 keluaran OpenAI, yang dilatih pada dataset 8 juta laman web.\n",
    "\n",
    "Secara khusus, kami memakai GPT-2 medium (345 juta parameter, 24 blok decoder-transformer, embedding size tiap token dimensinya 1024)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " https://anlp.jp/proceedings/annual_meeting/2017/pdf_dir/B7-5.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " https://www.jamez.it/blog/wp-content/uploads/2021/05/Haiku-Generation-A-Transformer-Based-Approach-With-\n",
    "Lots-Of-Control.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II - Baca dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kami melatih model kami menggunakan dataset haiku dari 2 sumber:\n",
    "\n",
    "- Kaggle (5 ribuan)\n",
    "-- https://docs.google.com/document/d/1aPM9Wu8xxjD8QX5fimu42QbMxVyBXHWInuXj8gt4DIg/edit?tab=t.0\n",
    "- Hugging Face (45 ribuan)\n",
    "-- https://huggingface.co/datasets/statworx/haiku\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Catatan:* Buat toggle TRAIN menjadi True untuk meng-enable proses finetuning model. Jika False, model akan di-load dari hasil finetune sebelumnya, jika ada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('dataset.parquet')\n",
    "df = df.rename(columns={\"text\":\"haiku\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contoh haiku yang tersimpan di dalam dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delicate savage. / You'll never hold the cinder. / But still you will burn.\n",
      "A splash and a cry. / Words pulled from the riverside. / Dryed in the hot sun.\n",
      "Steamy, mist rising. / Rocks receiving downward crash. / As the jungle weeps.\n",
      "You were broken glass. / But I touched you even though. / I knew it would hurt.\n",
      "Eyes dance with firelight. / The Moon and I are lovers. / The spiteful sun dies.\n",
      "I woke up today. / I wanted to write a song. / I wrote a haiku.\n",
      "Know when to quit, friend. / No time to waste in this space. / Live well to the end.\n",
      "Gazing upon plains. / A loving wind warms my nose. / The sun behind me.\n",
      "The lion limped out. / And flipped his tail at March when. / Fleecy frolicked in.\n",
      "Your words are wounding. / Your tongue, a bloody scalpel. / Your aim, perfection.\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "\tprint(df.iloc[i, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>haiku</th>\n",
       "      <th>text_phonemes</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keyword_phonemes</th>\n",
       "      <th>gruen_score</th>\n",
       "      <th>text_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5280</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>What did you order? / I forgot what I wanted. ...</td>\n",
       "      <td>waht dihd yuw aor|der / ay fer|gaat waht ay wa...</td>\n",
       "      <td>you order</td>\n",
       "      <td>yuw aor|der</td>\n",
       "      <td>0.898392</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1216</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>My lover is gone. / Won't you come back to my ...</td>\n",
       "      <td>may lah|ver axz gaon / wownt yuw kahm baek tax...</td>\n",
       "      <td>come back</td>\n",
       "      <td>kahm baek</td>\n",
       "      <td>0.896483</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9866</th>\n",
       "      <td>twaiku</td>\n",
       "      <td>I just remembered. / I'm working tomorrow nigh...</td>\n",
       "      <td>ay jhahst rax|mehm|berd / ihm wer|kaxng tax|ma...</td>\n",
       "      <td>working tomorrow</td>\n",
       "      <td>wer|kihng tax|maa|row</td>\n",
       "      <td>0.894500</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18866</th>\n",
       "      <td>twaiku</td>\n",
       "      <td>Your eyes deceive you. / An illusion fools you...</td>\n",
       "      <td>yaor ayz dax|siyv yuw / axn ax|luw|zhaxn fuwlz...</td>\n",
       "      <td>illusion fools</td>\n",
       "      <td>ax|luw|zhaxn fuwlz</td>\n",
       "      <td>0.893575</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Worker bees can leave. / Even drones can fly a...</td>\n",
       "      <td>wer|ker biyz kaen liyv / iy|vaxn drownz kaen f...</td>\n",
       "      <td>worker bees</td>\n",
       "      <td>wer|ker biyz</td>\n",
       "      <td>0.893551</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48902</th>\n",
       "      <td>haiku_data_2</td>\n",
       "      <td>Moonrise. / An owl swoops up. / Something.</td>\n",
       "      <td>muwn|rayz axn awl swuwps ahp sahm|thaxng</td>\n",
       "      <td>owl</td>\n",
       "      <td>awl</td>\n",
       "      <td>0.893530</td>\n",
       "      <td>Moonrise. An owl swoops up. Something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35750</th>\n",
       "      <td>haiku_data_1</td>\n",
       "      <td>Moonrise. / An owl swoops up. / Something.</td>\n",
       "      <td>muwn|rayz axn awl swuwps ahp sahm|thaxng</td>\n",
       "      <td>owl</td>\n",
       "      <td>awl</td>\n",
       "      <td>0.893530</td>\n",
       "      <td>Moonrise. An owl swoops up. Something.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Change begins right now. / Let music and dance...</td>\n",
       "      <td>cheynjh bax|gihnz rayt naw / leht myuw|zaxk ae...</td>\n",
       "      <td>dance commence</td>\n",
       "      <td>daens kax|mehns</td>\n",
       "      <td>0.893291</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Winter is coming. / Cold winds are blowing ove...</td>\n",
       "      <td>wihn|ter axz kah|maxng / kowld wihndz aar blow...</td>\n",
       "      <td>cold winds</td>\n",
       "      <td>kowld wihndz</td>\n",
       "      <td>0.891717</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2056</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Your coldness burns me. / You stab me with ici...</td>\n",
       "      <td>yaor kowld|naxs bernz miy / yuw staeb miy wihd...</td>\n",
       "      <td>icicles</td>\n",
       "      <td>ay|sax|kaxlz</td>\n",
       "      <td>0.891561</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             source                                              haiku  \\\n",
       "5280        bfbarry  What did you order? / I forgot what I wanted. ...   \n",
       "1216        bfbarry  My lover is gone. / Won't you come back to my ...   \n",
       "9866         twaiku  I just remembered. / I'm working tomorrow nigh...   \n",
       "18866        twaiku  Your eyes deceive you. / An illusion fools you...   \n",
       "717         bfbarry  Worker bees can leave. / Even drones can fly a...   \n",
       "48902  haiku_data_2         Moonrise. / An owl swoops up. / Something.   \n",
       "35750  haiku_data_1         Moonrise. / An owl swoops up. / Something.   \n",
       "3741        bfbarry  Change begins right now. / Let music and dance...   \n",
       "978         bfbarry  Winter is coming. / Cold winds are blowing ove...   \n",
       "2056        bfbarry  Your coldness burns me. / You stab me with ici...   \n",
       "\n",
       "                                           text_phonemes          keywords  \\\n",
       "5280   waht dihd yuw aor|der / ay fer|gaat waht ay wa...         you order   \n",
       "1216   may lah|ver axz gaon / wownt yuw kahm baek tax...         come back   \n",
       "9866   ay jhahst rax|mehm|berd / ihm wer|kaxng tax|ma...  working tomorrow   \n",
       "18866  yaor ayz dax|siyv yuw / axn ax|luw|zhaxn fuwlz...    illusion fools   \n",
       "717    wer|ker biyz kaen liyv / iy|vaxn drownz kaen f...       worker bees   \n",
       "48902           muwn|rayz axn awl swuwps ahp sahm|thaxng               owl   \n",
       "35750           muwn|rayz axn awl swuwps ahp sahm|thaxng               owl   \n",
       "3741   cheynjh bax|gihnz rayt naw / leht myuw|zaxk ae...    dance commence   \n",
       "978    wihn|ter axz kah|maxng / kowld wihndz aar blow...        cold winds   \n",
       "2056   yaor kowld|naxs bernz miy / yuw staeb miy wihd...           icicles   \n",
       "\n",
       "            keyword_phonemes  gruen_score  \\\n",
       "5280             yuw aor|der     0.898392   \n",
       "1216               kahm baek     0.896483   \n",
       "9866   wer|kihng tax|maa|row     0.894500   \n",
       "18866     ax|luw|zhaxn fuwlz     0.893575   \n",
       "717             wer|ker biyz     0.893551   \n",
       "48902                    awl     0.893530   \n",
       "35750                    awl     0.893530   \n",
       "3741         daens kax|mehns     0.893291   \n",
       "978             kowld wihndz     0.891717   \n",
       "2056            ay|sax|kaxlz     0.891561   \n",
       "\n",
       "                                    text_punc  \n",
       "5280                                     None  \n",
       "1216                                     None  \n",
       "9866                                     None  \n",
       "18866                                    None  \n",
       "717                                      None  \n",
       "48902  Moonrise. An owl swoops up. Something.  \n",
       "35750  Moonrise. An owl swoops up. Something.  \n",
       "3741                                     None  \n",
       "978                                      None  \n",
       "2056                                     None  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted_gruen = df.sort_values(by='gruen_score', ascending=False)\n",
    "df_sorted_gruen.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>haiku</th>\n",
       "      <th>text_phonemes</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keyword_phonemes</th>\n",
       "      <th>gruen_score</th>\n",
       "      <th>text_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Delicate savage. / You'll never hold the cinde...</td>\n",
       "      <td>deh|lax|kaxt sae|vaxjh / yuwl neh|ver hhowld d...</td>\n",
       "      <td>cinder</td>\n",
       "      <td>sihn|der</td>\n",
       "      <td>0.639071</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>A splash and a cry. / Words pulled from the ri...</td>\n",
       "      <td>ax splaesh aend ax kray / werdz puhld frahm dh...</td>\n",
       "      <td>the riverside</td>\n",
       "      <td>dhax rih|ver|sayd</td>\n",
       "      <td>0.563353</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Steamy, mist rising. / Rocks receiving downwar...</td>\n",
       "      <td>stiy|miy mihst ray|zaxng / raaks rax|siy|vaxng...</td>\n",
       "      <td>mist rising</td>\n",
       "      <td>mihst ray|zaxng</td>\n",
       "      <td>0.538326</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>You were broken glass. / But I touched you eve...</td>\n",
       "      <td>yuw wer brow|kaxn glaes / baht ay tahcht yuw i...</td>\n",
       "      <td>broken glass</td>\n",
       "      <td>brow|kaxn glaes</td>\n",
       "      <td>0.703446</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Eyes dance with firelight. / The Moon and I ar...</td>\n",
       "      <td>ayz daens wihdh faxr|layt / dhax muwn aend ay ...</td>\n",
       "      <td>eyes dance</td>\n",
       "      <td>ayz daens</td>\n",
       "      <td>0.830985</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source                                              haiku  \\\n",
       "0  bfbarry  Delicate savage. / You'll never hold the cinde...   \n",
       "1  bfbarry  A splash and a cry. / Words pulled from the ri...   \n",
       "2  bfbarry  Steamy, mist rising. / Rocks receiving downwar...   \n",
       "3  bfbarry  You were broken glass. / But I touched you eve...   \n",
       "4  bfbarry  Eyes dance with firelight. / The Moon and I ar...   \n",
       "\n",
       "                                       text_phonemes       keywords  \\\n",
       "0  deh|lax|kaxt sae|vaxjh / yuwl neh|ver hhowld d...         cinder   \n",
       "1  ax splaesh aend ax kray / werdz puhld frahm dh...  the riverside   \n",
       "2  stiy|miy mihst ray|zaxng / raaks rax|siy|vaxng...    mist rising   \n",
       "3  yuw wer brow|kaxn glaes / baht ay tahcht yuw i...   broken glass   \n",
       "4  ayz daens wihdh faxr|layt / dhax muwn aend ay ...     eyes dance   \n",
       "\n",
       "    keyword_phonemes  gruen_score text_punc  \n",
       "0           sihn|der     0.639071      None  \n",
       "1  dhax rih|ver|sayd     0.563353      None  \n",
       "2    mihst ray|zaxng     0.538326      None  \n",
       "3    brow|kaxn glaes     0.703446      None  \n",
       "4          ayz daens     0.830985      None  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49024, 7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat 49024 baris data (i.e. 49024 haiku unik) yang kita miliki. Mari bekerja!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III - Pisah train set & test set (test size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df.sample(n = int(0.2 * len(df)))\n",
    "df_train = df.loc[~df.index.isin(df_test.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39220, 7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9804, 7)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_HAIKUS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV - Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data kita ubah ke dalam format yang diterima oleh model GPT-2, dengan control code <|haiku|> untuk menandakan bahwa kalimat input berupa haiku (bukan kalimat biasa), <|line|> sebagai delimiter penanda line break, dan <|endoftext|> untuk menandakan akhir dari sekuens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_length diatur 1024 karena kita memakai GPT-2 medium (345 juta parameter, 24 blok decoder-transformer, embedding size tiap token dimensinya 1024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Haiku(Dataset):\n",
    "    def __init__(self, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "        self.haiku = []\n",
    "\n",
    "        for row in df[\"haiku\"]:\n",
    "            row_split = row.split(\" / \")  # delimiter yg dipakai\n",
    "\n",
    "            try:\n",
    "                to_encode = f\"<|haiku|>{row_split[0]}<|lineA|>/n{row_split[1]}<|lineB|>{row_split[2]}<|endoftext|>\"  # separate each line break with control codes\n",
    "                TEST_HAIKUS.append(to_encode)\n",
    "                tokenized_haiku = self.tokenizer.encode(\n",
    "                    to_encode,\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                )\n",
    "                self.haiku.append(torch.tensor(tokenized_haiku))\n",
    "            except:\n",
    "                print(row)\n",
    "\n",
    "        if truncate:\n",
    "            self.haiku = self.haiku[:20000]\n",
    "\n",
    "        self.haiku_count = len(self.haiku)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.haiku_count\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.haiku[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\n",
      "Blue Asters. / '\n",
      "Winter wind. / '\n",
      "Again,Iamthelasttoknowriptide. / '\n",
      "Acrossthefaceofachalkhorserookfacesrook. / '\n",
      "Comingoutoftheseawhationcewas. / '\n",
      "Whitewaterrafting'theriverin'myeyes' / '\n",
      "Outofthedepthsofthemountainbluebird. / '\n",
      "(ASquiggleofskinks)upthewallsunrise. / '\n",
      "Allthelongdaythebusinessofbees. / '\n",
      "Elniodustcolorsthedyingbee. / '\n",
      "Starlightwheretheyfellsoftbodiesofbogongmoths. / '\n",
      "Coldwindstherecedingtempoofrain. / '\n",
      "Theeastcoaststormin'hervoice. / '\n",
      "Fallleafscentofunsippedwhiskey. / '\n",
      "A windless skin, Amnesia. / '\n",
      "Foraslongasicanremembersoapbubbles. / '\n",
      "[ Catswithotherplanszengarden] / '\n",
      "Attherootofallthenothingness. / '\n",
      "Stillunpackingapairofwrensinthelonggrass. / '\n",
      "Smellofripeappleswheniwouldpracticehowtofly. / '\n",
      "[huskingcornshetalksofoldlovers] / '\n",
      "Almostautumnthehumofsunflowers. / '\n",
      "Allthesilenceofwhitecrosses. / '\n",
      "Winterrainthesmoothnessofakidneybean. / '\n",
      "Once'amilltown'only'mother'herenow' / '\n",
      "Snowfalling'throughthenight'acrow's'whitethroat' / '\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "haiku = Haiku(truncate=True, gpt2_type=\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Di atas, bisa dilihat kalau terdapat sejumlah baris dengan struktur yang invalid sehingga dibuang saja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V - Load pretrained models (as well as the tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accumulated batch size (since GPT2 is so big)\n",
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VI - Train for 10 epochs, each with 20.000 iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use AdamW for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selama fase warm-up, learning rate di-decrement secara linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, model, tokenizer,\n",
    "    batch_size=24, epochs=10, lr=2e-5,\n",
    "    max_seq_len=400, warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
    "    test_mode=False,save_model_on_epoch=False,\n",
    "):\n",
    "\n",
    "    acc_steps = 100\n",
    "    device=torch.device(\"cuda\")\n",
    "    model = model.cuda()\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "            input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IBDA\\.conda\\envs\\uasdl\\lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:43, 456.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "tensor(2.9757, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:43, 462.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "tensor(2.8350, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:43, 460.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 3\n",
      "tensor(2.3209, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:42, 475.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 4\n",
      "tensor(2.2451, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:42, 472.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 5\n",
      "tensor(2.1414, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:42, 471.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 6\n",
      "tensor(1.9168, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:42, 475.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 7\n",
      "tensor(1.8382, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:41, 478.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 8\n",
      "tensor(1.9281, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:38, 520.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 9\n",
      "tensor(2.0187, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:38, 517.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Train the model on the specific data we have\n",
    "if TRAIN:\n",
    "    model = train(haiku, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VII - Save model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    torch.save(model, 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIII - Let's start generating haikus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_length=30,  # maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.,\n",
    "):\n",
    "    # Check if CUDA is available and set the device accordingly\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Move the model to the appropriate device\n",
    "    model = model.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Move the prompt tensor to the appropriate device\n",
    "    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(entry_length):\n",
    "            outputs = model(generated, labels=generated)\n",
    "            loss, logits = outputs[:2]\n",
    "            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            logits[:, indices_to_remove] = -float(\"Inf\")\n",
    "\n",
    "            next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "            # Check for the end-of-text token\n",
    "            if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                break\n",
    "\n",
    "    # Move the generated tensor back to the CPU for decoding\n",
    "    output_list = list(generated.squeeze().cpu().numpy())\n",
    "    output_text = tokenizer.decode(output_list)\n",
    "    return output_text  # Return a single generated string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing the result\n",
    "\n",
    "Kita akan menggunakan model yang sudah difine-tune untuk men-generate haiku baru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|haiku|>Finally, it was.<|lineA|>/nAbout the safest place I've.<|lineB|>Ever been.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "x = generate(model, tokenizer, \"<|haiku|>Finally, it\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X - Evaluate perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrik evaluasi perpleksitas: berdasarkan kemampuan LLM (i.e. GPT-2) untuk memprediksi entri berikutnya berdasarkan entri sebelumnya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from perplexity import compare_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_perplexity(model, tokenizer, test_set, entry_length=30, top_p=0.8, temperature=1.0):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    def calculate_perplexity(text):\n",
    "\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, labels=input_ids)\n",
    "            loss = outputs.loss  # Cross-entropy loss\n",
    "\n",
    "        return torch.exp(loss).item()\n",
    "\n",
    "    def generate_haiku(first_two_words):\n",
    "        \n",
    "        new_model = torch.load('model.pt')\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        new_model = new_model.to(device)\n",
    "        new_model.eval()\n",
    "\n",
    "        generated = torch.tensor(tokenizer.encode(first_two_words)).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            for _ in range(entry_length):\n",
    "\n",
    "                device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                new_model = new_model.to(device)\n",
    "                new_model.eval()\n",
    "                outputs = new_model(generated, labels=generated)\n",
    "                logits = outputs.logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                # Apply top-p sampling\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = -float(\"Inf\")\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    break\n",
    "\n",
    "        output_list = list(generated.squeeze().cpu().numpy())\n",
    "        return tokenizer.decode(output_list)\n",
    "\n",
    "    test_set_perplexities = []\n",
    "    generated_haiku_perplexities = []\n",
    "\n",
    "    for haiku in test_set:\n",
    "        # print(haiku, type(haiku))\n",
    "        # Calculate perplexity for the original haiku\n",
    "        test_set_perplexities.append(calculate_perplexity(haiku))\n",
    "\n",
    "        # Generate haiku based on the first two words of the original haiku\n",
    "        first_two_words = \" \".join(haiku.split()[:2])\n",
    "        generated_haiku = generate_haiku(first_two_words)\n",
    "\n",
    "        # Calculate perplexity for the generated haiku\n",
    "        generated_haiku_perplexities.append(calculate_perplexity(generated_haiku))\n",
    "\n",
    "    # Calculate average perplexities\n",
    "    avg_test_set_perplexity = sum(test_set_perplexities) / len(test_set_perplexities)\n",
    "    avg_generated_perplexity = sum(generated_haiku_perplexities) / len(generated_haiku_perplexities)\n",
    "\n",
    "    return avg_test_set_perplexity, avg_generated_perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Compare perplexity\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m avg_test_perplexity, avg_generated_perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_perplexity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhaiku\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Test Set Perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_test_perplexity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage Generated Haikus Perplexity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_generated_perplexity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[32], line 63\u001b[0m, in \u001b[0;36mcompare_perplexity\u001b[1;34m(model, tokenizer, test_set, entry_length, top_p, temperature)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Generate haiku based on the first two words of the original haiku\u001b[39;00m\n\u001b[0;32m     62\u001b[0m first_two_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(haiku\u001b[38;5;241m.\u001b[39msplit()[:\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m---> 63\u001b[0m generated_haiku \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_haiku\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfirst_two_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Calculate perplexity for the generated haiku\u001b[39;00m\n\u001b[0;32m     66\u001b[0m generated_haiku_perplexities\u001b[38;5;241m.\u001b[39mappend(calculate_perplexity(generated_haiku))\n",
      "Cell \u001b[1;32mIn[32], line 19\u001b[0m, in \u001b[0;36mcompare_perplexity.<locals>.generate_haiku\u001b[1;34m(first_two_words)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_haiku\u001b[39m(first_two_words):\n\u001b[1;32m---> 19\u001b[0m     new_model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m     new_model \u001b[38;5;241m=\u001b[39m new_model\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\IBDA\\.conda\\envs\\uasdl\\lib\\site-packages\\torch\\serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile,\n\u001b[0;32m   1027\u001b[0m                      map_location,\n\u001b[0;32m   1028\u001b[0m                      pickle_module,\n\u001b[0;32m   1029\u001b[0m                      overall_storage\u001b[38;5;241m=\u001b[39moverall_storage,\n\u001b[0;32m   1030\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[0;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\IBDA\\.conda\\envs\\uasdl\\lib\\site-packages\\torch\\serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[0;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[1;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[0;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[0;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[0;32m   1443\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\IBDA\\.conda\\envs\\uasdl\\lib\\site-packages\\torch\\serialization.py:1408\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1407\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[1;32m-> 1408\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[1;32mc:\\Users\\IBDA\\.conda\\envs\\uasdl\\lib\\site-packages\\torch\\serialization.py:1373\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m   1371\u001b[0m     storage \u001b[38;5;241m=\u001b[39m overall_storage[storage_offset:storage_offset \u001b[38;5;241m+\u001b[39m numel]\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1373\u001b[0m     storage \u001b[38;5;241m=\u001b[39m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_storage_from_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_untyped_storage\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;66;03m# swap here if byteswapping is needed\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m byteorderdata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Compare perplexity\n",
    "avg_test_perplexity, avg_generated_perplexity = compare_perplexity(model, tokenizer, df_test[\"haiku\"])\n",
    "print(f\"Average Test Set Perplexity: {avg_test_perplexity}\")\n",
    "print(f\"Average Generated Haikus Perplexity: {avg_generated_perplexity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perpleksitas dari haiku yang di-generate model kami lebih rendah dari model GPT-2 tanpa pretraining. Artinya, kemungkinan haiku test di-generate dengan model yang kami finetune lebih tinggi daripada dengan model lama.\n",
    "\n",
    "Jadi, model yang di-finetune lebih efektif dalam men-generate haiku artistic yang sungguhan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sumber kajian untuk perpleksitas:\n",
    "- https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1224/reports/custom_116767424.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star our repo pls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/GeryYulianto/HaikuGenerator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uasdl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
